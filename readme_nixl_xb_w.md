# nixl_connector.py ä¸‹åŠéƒ¨åˆ†å®ç°
ä½ è´´çš„è¿™ä¸€å¤§æ®µä»£ç åŸºæœ¬ä¸Šå°±æ˜¯ **vLLM ä¸­çš„ NIXLConnectorWorker å®ç°**ï¼Œè´Ÿè´£ç®¡ç† **KV Cache çš„æ³¨å†Œã€ä¼ è¾“ã€å¼‚æ­¥æ¡æ‰‹å’Œå®ŒæˆçŠ¶æ€æ£€æµ‹**ã€‚æˆ‘å¸®ä½ æ¢³ç†ä¸€ä¸‹æ ¸å¿ƒé€»è¾‘å’ŒèŒè´£ï¼š

---

### **1. æˆå‘˜å˜é‡åˆå§‹åŒ–**

* `self.dst_num_blocks`ï¼šè®°å½•æ¯ä¸ª Engine çš„ block æ•°ã€‚
* `_registered_descs`ï¼šå·²ç»æ³¨å†Œåˆ° NIXL çš„ memory descriptorsã€‚
* `_recving_metadata / _recving_transfers`ï¼šè·Ÿè¸ªæ¥æ”¶ä¸­çš„è¯·æ±‚å…ƒæ•°æ®å’Œä¼ è¾“å¥æŸ„ã€‚
* `_reqs_to_send`ï¼šç­‰å¾…å‘é€ä½†è¿˜æ²¡è§¦å‘æˆ–å®Œæˆçš„è¯·æ±‚ï¼ˆå¸¦è¶…æ—¶ï¼‰ã€‚
* `_nixl_handshake_listener_t / _handshake_initiation_executor`ï¼šZMQ æ¡æ‰‹çº¿ç¨‹ & å¼‚æ­¥çº¿ç¨‹æ± ã€‚
* `_ready_requests`ï¼šå·²ç»å®Œæˆæ¡æ‰‹ã€å¯ä»¥å¼€å§‹æ‹‰å–æ•°æ®çš„è¯·æ±‚ã€‚
* `_handshake_futures`ï¼šengine\_id â†’ futureï¼Œç”¨æ¥ç­‰å¾… handshake å®Œæˆã€‚
* `_handshake_lock`ï¼šä¿æŠ¤ `_handshake_futures` å’Œ `_remote_agents` çš„é”ã€‚

---

### **2. KV Cache æ³¨å†Œ**

`register_kv_caches` åšçš„äº‹ï¼š

* æ£€æŸ¥ KV Cache çš„ dtype / layoutï¼ˆNHD, HND, MLA ç­‰æƒ…å†µï¼‰ã€‚
* æ ¹æ® `cuda / tpu` ä¸åŒè®¾å¤‡ç±»å‹ï¼Œè®¡ç®— **slot\_size\_bytes / block\_len**ã€‚
* ä¸ºæ¯ä¸ª KV cache tensor æ„é€  **(base\_addr, region\_len, device\_id)**ï¼Œå¹¶æ³¨å†Œåˆ° NIXLã€‚
* ä¸ºæ¯ä¸ª block ç”Ÿæˆ **xfer descs**ï¼Œå‡†å¤‡å¥½åç»­ä¼ è¾“æè¿°ç¬¦ã€‚
* å¯åŠ¨ **ZMQ ROUTER çº¿ç¨‹**ï¼Œç›‘å¬å…¶ä»–è¿›ç¨‹è¯·æ±‚å…ƒæ•°æ®ï¼ˆæ¡æ‰‹ç¯èŠ‚ï¼‰ã€‚

---

### **3. NIXL Handshake æµç¨‹**

æœ‰ä¸¤ç§è§’è‰²ï¼š

* **listener**ï¼šæ¯ä¸ª worker èµ·ä¸€ä¸ª ZMQ ROUTERï¼Œç­‰å¾…åˆ«äºº `GET_META_MSG`ï¼Œè¿”å›è‡ªå·± metadataã€‚
* **initiator**ï¼šæœ¬åœ° worker éœ€è¦ remote KVï¼Œå°±è°ƒç”¨ `_nixl_handshake` â†’ é€šè¿‡ `ZMQ REQ` å‘è¿œç«¯å‘ `GET_META_MSG` â†’ æ”¶åˆ°è¿œç«¯ `NixlAgentMetadata` â†’ æ³¨å†Œ remote agentã€‚

è¿™é‡Œçš„å¼‚æ­¥åŒ–ï¼š

* `_background_nixl_handshake` ä¼šæäº¤ç»™çº¿ç¨‹æ± ï¼Œfuture å®Œæˆåï¼š

  * æ›´æ–° `_remote_agents`ã€‚
  * æŠŠ `(req_id, meta)` æ”¾è¿› `_ready_requests`ï¼Œä¾› `start_load_kv` æ¶ˆè´¹ã€‚

---

### **4. è¯»å†™ KV Cache**

* **å†™ï¼ˆprefill é˜¶æ®µï¼‰**ï¼šæŠŠæœ¬åœ° KV Cache æ³¨å†Œæˆ NIXL çš„ region/blocksï¼Œå…¶ä»– worker å¯ä»¥æ‹‰å–ã€‚
* **è¯»ï¼ˆdecode é˜¶æ®µï¼‰**ï¼š

  * `start_load_kv` æ£€æŸ¥ handshake æ˜¯å¦å®Œæˆï¼Œå¦‚æœæ²¡å®Œæˆ â†’ å‘èµ· handshakeã€‚
  * å¦‚æœ handshake å®Œæˆ â†’ `_read_blocks_for_req` è§¦å‘ **éé˜»å¡ NIXL xfer**ã€‚
  * `_recving_transfers` è·Ÿè¸ªå¥æŸ„ï¼Œå®šæœŸè½®è¯¢çŠ¶æ€ã€‚

---

### **5. Host Buffer ç›¸å…³**

å› ä¸º NIXL åªæ”¯æŒä¸€éƒ¨åˆ†è®¾å¤‡ï¼ˆGPU/DRAMï¼‰ï¼ŒTPU ç­‰è®¾å¤‡è¦èµ° CPU ä¸­è½¬ï¼š

* `initialize_host_xfer_buffer`ï¼šåœ¨ CPU ä¸Šåˆ†é… tensorã€‚
* `set_host_xfer_buffer_ops`ï¼šè®¾ç½® d2h/h2d copy æ“ä½œã€‚
* `sync_recved_kv_to_device`ï¼šä» host buffer æ‹·åˆ° deviceã€‚
* `save_kv_to_host`ï¼šä» device æ‹·åˆ° host bufferã€‚

---

### **6. çŠ¶æ€æ£€æŸ¥ & æ¸…ç†**

* `get_finished`ï¼š

  * è½®è¯¢ `_get_new_notifs`ï¼ˆè¿œç«¯è¯»å®Œæˆé€šçŸ¥ï¼‰ã€‚
  * è½®è¯¢ `_pop_done_transfers`ï¼ˆæ£€æŸ¥æœ¬åœ° async xfer çŠ¶æ€ï¼‰ã€‚
  * è¶…æ—¶è¯·æ±‚ç›´æ¥é‡Šæ”¾ã€‚
  * å¦‚æœæœ‰ host bufferï¼Œåˆ™åœ¨å®Œæˆæ—¶åŒæ­¥åˆ° deviceã€‚

* `__del__`ï¼šææ„æ—¶å…³é—­ handshake executorï¼Œå›æ”¶çº¿ç¨‹ã€‚

---

### **7. å¼‚æ„ TPï¼ˆheterogeneous tensor parallelismï¼‰**

ä»£ç é‡Œæœ‰ä¸€å¤§æ®µæ³¨é‡Šä¸“é—¨è®²è§£ï¼š

* **Homogeneous TP**ï¼šD-worker ä» P-worker å¯¹åº” rank æ‹‰å–æ•´å— KVã€‚
* **Heterogeneous TP**ï¼šD-worker world\_size > P-worker world\_sizeï¼Œå¤šä¸ª D-worker å…±äº«ä¸€ä¸ª P-worker çš„ KV Cacheï¼Œéœ€è¦æ²¿ç€ `kv_head` ç»´åº¦åˆ‡åˆ†ã€‚
* MLA æ¨¡å‹ä¾‹å¤–ï¼šKV Cache å¤åˆ¶ï¼Œä¸åˆ‡åˆ†ã€‚

---

### **æ ¸å¿ƒè¦ç‚¹æ€»ç»“**

è¿™æ®µä»£ç çš„å…³é”®ä½œç”¨ï¼š

1. **æ³¨å†Œ**æœ¬åœ° KV Cache åˆ° NIXLï¼ˆregion/block descsï¼‰ã€‚
2. **ç›‘å¬**è¿œç«¯ handshake è¯·æ±‚ï¼Œè¿”å› metadataã€‚
3. **å‘èµ·** handshakeï¼Œæ‹¿åˆ°è¿œç«¯ KV çš„åœ°å€ & layoutã€‚
4. **å¼‚æ­¥ä¼ è¾“**ï¼šéé˜»å¡è§¦å‘ NIXL xferï¼Œåå°è½®è¯¢å®ŒæˆçŠ¶æ€ã€‚
5. **æ”¯æŒ heterogeneous TP**ï¼šåœ¨ KV head ç»´åº¦ä¸Šåˆ‡åˆ†ã€‚
6. **æ”¯æŒ TPU/host buffer fallback**ï¼šCPU mem ä¸­è½¬ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **æ—¶åºå›¾**ï¼ˆPrefill Worker â†” Decode Worker ä¹‹é—´ handshake + KV xfer æµç¨‹ï¼‰ï¼Ÿè¿™æ ·ä½ èƒ½æ›´ç›´è§‚åœ°çœ‹åˆ° **ZMQ + NIXL è°ƒç”¨é“¾**ã€‚



# æ³¨æ„åŠ›ç®€ä»‹
ä½ è´´çš„è¿™æ®µä»£ç æ˜¯ **vLLM** é‡Œ Attentionï¼ˆæ³¨æ„åŠ›ï¼‰å®ç°çš„åç«¯é€‰æ‹©é€»è¾‘ã€‚æˆ‘ä»¬é€è¡Œæ‹†å¼€ï¼š

```python
backend = get_attn_backend(
    self.model_config.get_head_size(),        # æ¯ä¸ª attention head çš„ç»´åº¦å¤§å°
    self.model_config.dtype,                  # æ¨¡å‹çš„å‚æ•°ç²¾åº¦ (fp16/bf16/fp32)
    self.cache_config.cache_dtype,            # KV Cache çš„å­˜å‚¨ç²¾åº¦
    self.block_size,                          # è§£ç æ—¶ block çš„å¤§å° (token block æ•°é‡)
    self.model_config.is_attention_free,      # æ¨¡å‹æ˜¯å¦æ˜¯ attention-freeï¼ˆæ¯”å¦‚ MLP-only, Linear Attentionï¼‰
    use_mla=self.use_mla                      # æ˜¯å¦ä½¿ç”¨ MLA (Multi-head Latent Attention) ä¼˜åŒ–
)
self.backend_name = backend.get_name()
```

---

### ğŸ”‘ å…³é”®ç‚¹è§£é‡Š

1. **Attention çš„æ ¸å¿ƒä»»åŠ¡**

   * è¾“å…¥ `Q, K, V` ä¸‰ä¸ªå¼ é‡ï¼Œè®¡ç®—

     $$
     \text{Attention}(Q,K,V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
     $$
   * åœ¨å¤§æ¨¡å‹æ¨ç†é‡Œï¼ŒAttention æ˜¯æœ€è€—æ—¶ã€æœ€è€—æ˜¾å­˜çš„æ¨¡å— â†’ ç‰¹åˆ«æ˜¯ KV cache è®¿é—®å’ŒçŸ©é˜µä¹˜æ³•ã€‚

2. **get\_attn\_backend(...)**
   vLLM æ ¹æ®é…ç½®é€‰æ‹©ä¸åŒçš„ Attention å†…æ ¸ï¼ˆbackendï¼‰ã€‚å¸¸è§ backend æœ‰ï¼š

   * **FlashAttention**ï¼ˆCUDA kernelï¼Œé«˜æ•ˆ IOï¼Œçœæ˜¾å­˜ï¼‰
   * **PagedAttention**ï¼ˆvLLM è‡ªç ”ï¼Œæ”¯æŒ KV cache åˆ†å—ç®¡ç†ï¼ŒèŠ‚çœå†…å­˜ï¼Œæ”¯æŒåŠ¨æ€ context lengthï¼‰
   * **Fused Attention**ï¼ˆæ¯”å¦‚ cutlass/cublas èåˆå®ç°ï¼‰
   * **MLA (Multi-head Latent Attention)** â†’ ä¸€ç§æ”¹è¿›ç‰ˆçš„ Attentionï¼Œå‡å°‘è®¡ç®—å’Œæ˜¾å­˜å¼€é”€

   é€‰æ‹©é€»è¾‘å–å†³äºï¼š

   * `head_size`ï¼ˆhead ç»´åº¦å¤§å°å†³å®š kernel æ”¯æŒæ€§ï¼Œæ¯”å¦‚ FlashAttention è¦æ±‚ 64 çš„å€æ•°ï¼‰
   * `dtype`ï¼ˆfp16/bf16 æ”¯æŒæ›´å¥½ï¼Œfp32 å¯èƒ½é€€åŒ–åˆ° naive kernelï¼‰
   * `cache_dtype`ï¼ˆKV Cache çš„ç²¾åº¦ï¼Œæ¯”å¦‚ fp16/int8ï¼‰
   * `block_size`ï¼ˆæ˜¯å¦éœ€è¦åˆ†å— KV Cacheï¼‰
   * `is_attention_free`ï¼ˆå¦‚æœæ¨¡å‹æ ¹æœ¬ä¸éœ€è¦ Attentionï¼Œç›´æ¥è¿”å› dummy backendï¼‰
   * `use_mla`ï¼ˆæ˜¯å¦å¯ç”¨ MLA ä¼˜åŒ–ï¼‰

3. **backend.get\_name()**
   ä¼šè¿”å›å½“å‰ä½¿ç”¨çš„ Attention å†…æ ¸åå­—ï¼Œæ¯”å¦‚ï¼š

   * `"flash_attn"`
   * `"paged_attn"`
   * `"fused_attn"`
   * `"mla_attn"`
   * `"naive_attn"`

---

âœ… æ€»ç»“ä¸€å¥ï¼š
`attn` è¿™é‡ŒæŒ‡çš„æ˜¯ **æ¨¡å‹æ¨ç†ä¸­æ³¨æ„åŠ›è®¡ç®—çš„æ‰§è¡Œåç«¯**ï¼Œ`get_attn_backend` ä¼šæ ¹æ®æ¨¡å‹ head sizeã€ç²¾åº¦ã€KV cache è®¾ç½®ç­‰æ¡ä»¶ï¼Œé€‰æ‹©åˆé€‚çš„ Attention kernelï¼Œå®ç°æ¨ç†æ—¶çš„æ€§èƒ½/æ˜¾å­˜ä¼˜åŒ–ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **vLLM é‡Œ Attention backend é€‰æ‹©æµç¨‹å›¾**ï¼ˆæ¡ä»¶ â†’ backendï¼‰ï¼Ÿè¿™æ ·ä½ å°±èƒ½ä¸€çœ¼çœ‹å‡ºä»£ç è·‘åˆ°å“ªä¸ªåˆ†æ”¯ã€‚




# KVConnectorBase_V1åŸºç±»


